{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data prep & CNN pipeline for the Arabic **words** model  \n",
        "\n",
        "This block mounts Google Drive, extracts the words dataset, preprocesses raw WAV files into fixed-length normalized waveforms, encodes labels, and defines a 1D CNN architecture for training a multi-class Arabic word classifier.  \n",
        "\n",
        "- **Mount & extract:** Mounts Drive, locates `arabic_words_dataset.zip`, unzips into `/content/arabic_words_dataset`, and lists top-level directories to confirm dataset structure.  \n",
        "- **Dataset layout & classes:** Scans each subfolder (one per word class). Prints number of `.wav` files per class to verify balance.  \n",
        "- **Audio preprocessing:**  \n",
        "  - **Loading:** Uses `librosa.load()` at **16 kHz** input.  \n",
        "  - **Resampling:** Downsamples to **8 kHz** (`SR_MODEL=8000`) for lighter models.  \n",
        "  - **Fixed length:** Pads/truncates each clip to exactly **8000 samples** (`FIX_LEN=8000` ‚âà 1 sec).  \n",
        "  - **Normalization:** Applies per-sample standardization (`(x ‚Äì mean) / std`).  \n",
        "  - **Final shape:** Stores as `(num_samples, 8000, 1)` for Conv1D input.  \n",
        "- **Labels & encoding:**  \n",
        "  - Collects `all_label` list, encodes with `LabelEncoder`, and converts to **one-hot vectors** via `to_categorical`.  \n",
        "  - Prints full class list (truncated if long).  \n",
        "- **Train/validation split:** Uses `train_test_split` with **stratification** to maintain class balance. Splits **80/20** with reproducible seed (`SEED=777`). Reports shapes of `X_train`, `X_val`.  \n",
        "- **Model architecture (Conv1D CNN):**  \n",
        "  - **Input:** (8000, 1).  \n",
        "  - **Conv blocks:** Four Conv1D layers with filters `[8, 16, 32, 64]` and kernel sizes `[13, 11, 9, 7]`. Each block has: Conv ‚Üí BatchNorm ‚Üí MaxPool1D ‚Üí Dropout(0.3).  \n",
        "  - **Pooling:** GlobalAveragePooling1D for temporal aggregation.  \n",
        "  - **Dense layers:** Dense(256) ‚Üí Dropout(0.3) ‚Üí Dense(128) ‚Üí Dropout(0.3).  \n",
        "  - **Output:** Dense(#classes, softmax).  \n",
        "- **Optimizer & compile:**  \n",
        "  - Attempts AdamW (`lr=1e-3, weight_decay=1e-4`), falls back to Adam if unavailable.  \n",
        "  - Loss: categorical crossentropy.  \n",
        "  - Metric: accuracy.  \n",
        "  - Model summary confirms layer stack and parameter counts.  \n",
        "\n",
        "**Key hyperparameters:** `SR_LOAD=16000`, `SR_MODEL=8000`, `FIX_LEN=8000`, `SEED=777`, conv kernel sizes `[13,11,9,7]`, dropout rate `0.3`, Dense layers `[256,128]`, optimizer `AdamW/Adam (lr=1e-3)`.  \n",
        "\n",
        "**Purpose:** Prepares Arabic word recordings into standardized 8kHz waveforms and trains a compact but deep 1D CNN for multi-class word recognition, forming the basis for the app‚Äôs higher-level recitation feedback.  "
      ],
      "metadata": {
        "id": "wmOqWV61S5Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, files\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import glob\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "from scipy.fftpack import fft\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, BatchNormalization, GlobalAveragePooling1D, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "fbXZIP5ygypO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUeBZ7ziSLdZ"
      },
      "outputs": [],
      "source": [
        "SEED = 777\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/QariAI/arabic_words_dataset.zip\"\n",
        "\n",
        "!mkdir -p /content/arabic_words_dataset\n",
        "!unzip -q \"$zip_path\" -d /content/arabic_words_dataset\n",
        "\n",
        "DATASET_PATH = \"/content/arabic_words_dataset\"\n",
        "print(\"üìÇ Unzipped dataset contents:\")\n",
        "print(os.listdir(DATASET_PATH)[:10])\n"
      ],
      "metadata": {
        "id": "6Q6Tjji4Tapj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SR_LOAD = 16000\n",
        "SR_MODEL = 8000\n",
        "FIX_LEN = 8000\n",
        "\n",
        "def norm_wave(w):\n",
        "    w = np.asarray(w, dtype=np.float32)\n",
        "    m, s = w.mean(), w.std()\n",
        "    return (w - m) / (s + 1e-8)\n",
        "\n",
        "class_audio_train = sorted([d for d in os.listdir(DATASET_PATH) if os.path.isdir(os.path.join(DATASET_PATH, d))])\n",
        "print(f\"üîé Found {len(class_audio_train)} classes:\")\n",
        "print(class_audio_train)\n",
        "\n",
        "all_wave = []\n",
        "all_label = []\n",
        "\n",
        "for label in class_audio_train:\n",
        "    folder = os.path.join(DATASET_PATH, label)\n",
        "    wavs = [f for f in os.listdir(folder) if f.lower().endswith(\".wav\")]\n",
        "    print(f\"‚è≥ Loading '{label}' ({len(wavs)} files) ...\")\n",
        "\n",
        "    for fname in wavs:\n",
        "        fpath = os.path.join(folder, fname)\n",
        "        try:\n",
        "            y, sr = librosa.load(fpath, sr=SR_LOAD)\n",
        "            y8 = librosa.resample(y, orig_sr=SR_LOAD, target_sr=SR_MODEL)\n",
        "            if len(y8) < FIX_LEN:\n",
        "                pad = FIX_LEN - len(y8)\n",
        "                y8 = np.pad(y8, (0, pad), mode=\"constant\")\n",
        "            elif len(y8) > FIX_LEN:\n",
        "                y8 = y8[:FIX_LEN]\n",
        "\n",
        "            y8 = norm_wave(y8)\n",
        "            all_wave.append(y8)\n",
        "            all_label.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Skipped {fname}: {e}\")\n",
        "\n",
        "all_wave = np.array(all_wave, dtype=np.float32).reshape(-1, FIX_LEN, 1)\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_idx = le.fit_transform(all_label)\n",
        "classes = list(le.classes_)\n",
        "y = to_categorical(y_idx, num_classes=len(classes))\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    all_wave, y, stratify=y_idx, test_size=0.20, random_state=777, shuffle=True\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Preprocess complete:\")\n",
        "print(f\" - Total samples: {len(all_wave)}\")\n",
        "print(f\" - Train: {X_train.shape}, Val: {X_val.shape}\")\n",
        "print(f\" - Classes: {len(classes)} -> {classes[:10]}{' ...' if len(classes) > 10 else ''}\")\n"
      ],
      "metadata": {
        "id": "MWBHSrufT-2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üîß TF version:\", tf.__version__)\n",
        "print(\"üß† GPU:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "inputs = Input(shape=(8000, 1))\n",
        "\n",
        "x = Conv1D(8, 13, padding='valid', activation='relu', strides=1)(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling1D(3)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "x = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling1D(3)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "x = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling1D(3)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "x = Conv1D(64, 7, padding='valid', activation='relu', strides=1)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling1D(3)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "outputs = Dense(len(classes), activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "try:\n",
        "    optimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=1e-3, weight_decay=1e-4)\n",
        "except Exception as e:\n",
        "    print(\"‚ÑπÔ∏è AdamW not available, falling back to Adam:\", e)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "bo5ql77SUPnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Training & evaluation for the Arabic **words** model  \n",
        "\n",
        "This block trains the Conv1D CNN using the prepared waveform dataset, applying callbacks for early stopping, dynamic learning rate adjustment, and checkpointing.  \n",
        "\n",
        "- **Early stopping:**  \n",
        "  - Monitors `val_loss`, mode = `min`.  \n",
        "  - Stops training if no improvement after **10 epochs** with at least `1e-4` delta.  \n",
        "  - Restores the best weights automatically.  \n",
        "\n",
        "- **Learning rate scheduling (ReduceLROnPlateau):**  \n",
        "  - Monitors `val_loss`.  \n",
        "  - Reduces learning rate by factor **0.5** if no improvement for 4 epochs.  \n",
        "  - Minimum LR enforced at `1e-5`.  \n",
        "\n",
        "- **Checkpointing:**  \n",
        "  - Saves the model to `/content/best_model.h5`.  \n",
        "  - Uses `val_accuracy` as criterion (highest value kept).  \n",
        "\n",
        "- **Model training:**  \n",
        "  - Runs up to **50 epochs** with `batch_size=64`.  \n",
        "  - Trains on `(X_train, y_train)` with validation on `(X_val, y_val)`.  \n",
        "  - All three callbacks (`es`, `plateau`, `ckpt`) applied.  \n",
        "  - Prints confirmation of completion and path of best saved model.  \n",
        "\n",
        "- **Best epoch tracking:**  \n",
        "  - Reports the epoch index with highest `val_accuracy`.  \n",
        "  - Displays `val_accuracy` and `val_loss` values at that epoch.  \n",
        "\n",
        "**Purpose:** Provides robust model training with automatic prevention of overfitting, dynamic learning rate scheduling, and safe saving of the best model for deployment.  \n"
      ],
      "metadata": {
        "id": "TMc2IwABTAus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    patience=10,\n",
        "    min_delta=1e-4,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "plateau = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=4,\n",
        "    min_lr=1e-5,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "ckpt_path = \"/content/best_model.h5\"\n",
        "ckpt = ModelCheckpoint(\n",
        "    ckpt_path,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[es, plateau, ckpt],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training complete. Best model saved to:\", ckpt_path)\n",
        "best_epoch = int(np.argmax(history.history['val_accuracy'])) + 1\n",
        "print(f\"üîù Best epoch: {best_epoch} | \"\n",
        "      f\"val_accuracy={history.history['val_accuracy'][best_epoch-1]:.4f} | \"\n",
        "      f\"val_loss={history.history['val_loss'][best_epoch-1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "JyrvyzemUdvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Evaluation & Latency for the Arabic **words** model  \n",
        "\n",
        "This block reloads the best saved checkpoint, evaluates the CNN on the validation set, and measures runtime performance.  \n",
        "\n",
        "- **Validation metrics:**  \n",
        "  - Computes predictions on the held-out validation set.  \n",
        "  - Reports global accuracy, confusion matrix, and classification report (precision, recall, F1).  \n",
        "  - Plots per-class accuracy to highlight strengths and weaknesses.  \n",
        "  - Prints the 5 lowest-performing classes.  \n",
        "  - Calculates Top-1 and Top-3 accuracy to assess near-miss performance.  \n",
        "\n",
        "- **Latency benchmarking:**  \n",
        "  - Measures **single-sample latency** (ms per utterance) to simulate real-time app use.  \n",
        "  - Measures **batched latency** (per-sample at batch size 64) for throughput analysis.  \n",
        "  - Confirms model inference is suitable for interactive pronunciation feedback (~1 second clips).  \n",
        "\n",
        "**Purpose:** Provides a comprehensive assessment of both model accuracy and speed, ensuring the Arabic words classifier is effective and responsive enough for integration into the mobile learning application.  \n"
      ],
      "metadata": {
        "id": "2w74tsKWTktg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = tf.keras.models.load_model(\"/content/best_model.h5\")\n",
        "\n",
        "Y_pred = best_model.predict(X_val, batch_size=128, verbose=0)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "y_true = np.argmax(y_val, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,12))\n",
        "disp.plot(ax=ax, cmap=\"Blues\", xticks_rotation=90, colorbar=False)\n",
        "plt.title(\"Confusion Matrix (Validation)\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes, digits=3))\n",
        "\n",
        "acc = (y_pred == y_true).mean()\n",
        "print(f\"\\n‚úÖ Validation Accuracy (recomputed): {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "anj_9T87WoSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "per_class_total = cm.sum(axis=1)\n",
        "per_class_correct = np.diag(cm)\n",
        "per_class_acc = per_class_correct / np.maximum(per_class_total, 1)\n",
        "\n",
        "order = np.argsort(per_class_acc)\n",
        "sorted_acc = per_class_acc[order]\n",
        "sorted_names = [classes[i] for i in order]\n",
        "\n",
        "plt.figure(figsize=(10, 14))\n",
        "plt.barh(sorted_names, sorted_acc)\n",
        "plt.xlabel(\"Per-class Accuracy\")\n",
        "plt.title(\"Per-class Accuracy (Validation Set)\")\n",
        "plt.xlim(0, 1.0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üîé 5 Worst-performing classes:\")\n",
        "for i in range(5):\n",
        "    idx = order[i]\n",
        "    print(f\" - {classes[idx]}: {per_class_acc[idx]:.3f} (correct {cm[idx, idx]}/{per_class_total[idx]})\")\n",
        "\n",
        "def top_k_accuracy(probs, true_idx, k=3):\n",
        "    topk = np.argsort(probs, axis=1)[:, -k:]\n",
        "    hits = np.any(topk == true_idx[:, None], axis=1)\n",
        "    return hits.mean()\n",
        "\n",
        "top1 = (y_pred == y_true).mean()\n",
        "top3 = top_k_accuracy(Y_pred, y_true, k=3)\n",
        "\n",
        "print(f\"\\nTop-1 Accuracy: {top1:.4f}\")\n",
        "print(f\"Top-3 Accuracy: {top3:.4f}\")\n"
      ],
      "metadata": {
        "id": "x8hChlUyW6Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best = tf.keras.models.load_model(\"/content/best_model.h5\")\n",
        "\n",
        "def bench_single_sample(model, X, warmup=20, iters=200):\n",
        "    model.predict(X[:warmup], batch_size=64, verbose=0)\n",
        "    t0 = time.time()\n",
        "    for i in range(iters):\n",
        "        model.predict(X[i:i+1], batch_size=1, verbose=0)\n",
        "    t1 = time.time()\n",
        "    per = (t1 - t0) / iters\n",
        "    return per\n",
        "\n",
        "def bench_batch(model, X, batch_size=64, iters=10):\n",
        "    model.predict(X[:batch_size], batch_size=batch_size, verbose=0)\n",
        "    t0 = time.time()\n",
        "    for i in range(iters):\n",
        "        model.predict(X[:batch_size], batch_size=batch_size, verbose=0)\n",
        "    t1 = time.time()\n",
        "    per_batch = (t1 - t0) / iters\n",
        "    return per_batch / batch_size\n",
        "\n",
        "Xslice = X_val[:256]\n",
        "\n",
        "ss = bench_single_sample(best, Xslice, warmup=32, iters=128)\n",
        "bs = bench_batch(best, Xslice, batch_size=64, iters=20)\n",
        "\n",
        "print(f\"üïí Single-sample avg latency: {ss*1000:.2f} ms  (~{1.0/ss:.1f} samples/sec)\")\n",
        "print(f\"üïí Batched avg latency (per sample @64): {bs*1000:.2f} ms\")\n",
        "print(\"Note: GPU on Colab can vary; CPU typically higher latency but similar ordering.\")\n"
      ],
      "metadata": {
        "id": "9LFMH4PoXOHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BEST_PATH = \"/content/best_model.h5\"\n",
        "best = tf.keras.models.load_model(BEST_PATH)\n",
        "\n",
        "try:\n",
        "    classes\n",
        "except NameError:\n",
        "    DATASET_PATH = \"/content/arabic_words_dataset\"\n",
        "    classes = sorted([d for d in os.listdir(DATASET_PATH) if os.path.isdir(os.path.join(DATASET_PATH, d))])\n",
        "\n",
        "SR_LOAD = 16000\n",
        "SR_MODEL = 8000\n",
        "FIX_LEN = 8000\n",
        "\n",
        "def norm_wave(w):\n",
        "    w = np.asarray(w, dtype=np.float32)\n",
        "    m, s = w.mean(), w.std()\n",
        "    return (w - m) / (s + 1e-8)\n",
        "\n",
        "def preprocess_wave_path(path, sr_load=SR_LOAD, sr_model=SR_MODEL, fix_len=FIX_LEN):\n",
        "    \"\"\"Load any .wav, convert to mono 16k, resample to 8k, pad/trim to 1s, normalize, add dims.\"\"\"\n",
        "    y, sr = librosa.load(path, sr=sr_load, mono=True)\n",
        "    y8 = librosa.resample(y, orig_sr=sr, target_sr=sr_model)\n",
        "    if len(y8) < fix_len:\n",
        "        y8 = np.pad(y8, (0, fix_len - len(y8)), mode=\"constant\")\n",
        "    elif len(y8) > fix_len:\n",
        "        y8 = y8[:fix_len]\n",
        "    y8 = norm_wave(y8).reshape(1, fix_len, 1)\n",
        "    return y8\n",
        "\n",
        "def predict_file(path):\n",
        "    \"\"\"Return (pred_label, pred_conf, top5_dict) for a given .wav path.\"\"\"\n",
        "    x = preprocess_wave_path(path)\n",
        "    probs = best.predict(x, verbose=0)[0]\n",
        "    top = int(np.argmax(probs))\n",
        "    pred_label = classes[top]\n",
        "    pred_conf = float(probs[top])\n",
        "    top5_idx = np.argsort(probs)[-5:][::-1]\n",
        "    top5 = {classes[i]: float(probs[i]) for i in top5_idx}\n",
        "    return pred_label, pred_conf, top5\n",
        "\n",
        "print(\"‚úÖ Model & helpers ready. Classes:\", len(classes))\n"
      ],
      "metadata": {
        "id": "Pcm_F9ujbP4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üì§ Select one or more .wav files to test (ideally ~1 second).\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fname, _ in uploaded.items():\n",
        "    path = f\"/content/{fname}\"\n",
        "    try:\n",
        "        label, conf, top5 = predict_file(path)\n",
        "        print(\"\\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "        print(f\"üéß {fname}\")\n",
        "        display(Audio(path))\n",
        "        print(f\"üîÆ Prediction: {label}  ({conf:.2%})\")\n",
        "        print(\"üèÖ Top‚Äë5:\")\n",
        "        for k, v in top5.items():\n",
        "            print(f\"   - {k:10s}: {v:.2%}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error on {fname}: {e}\")\n"
      ],
      "metadata": {
        "id": "bF4vUet9bTRm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
