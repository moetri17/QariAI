{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kls1GHSX0I6B"
      },
      "source": [
        "# 1. Data prep & TF input pipeline for the Arabic **letters** model\n",
        "\n",
        "This block mounts Google Drive, locates and extracts the dataset zip, enumerates classes, and builds an efficient TensorFlow `tf.data` pipeline that turns raw WAV files into normalized log-mel spectrogram tensors ready for model training.\n",
        "\n",
        "- **Mount & extract:** Mounts Drive, finds `arabic_letters_dataset.zip`, extracts it to `/content/hijaiyah_data`, and lists top-level contents for a quick sanity check.  \n",
        "- **Dataset layout & counts:** Sets `BASE_PATH = \"/content/hijaiyah_data/Dataset Telkom 16KHz\"`. Iterates through class folders (`\"1\"` … `\"28\"`), counts `.wav` files per class, and prints totals—useful for verifying class balance.  \n",
        "- **Class mapping:** Defines a stable mapping from numeric folder IDs (`\"1\"`–`\"28\"`) to ordered labels (`01_A`, `02_Ba`, …, `28_Ya`). Builds parallel arrays `file_paths` and integer `labels` (0-indexed) and reports per-class sample counts plus a few sample (path, label) pairs.  \n",
        "- **Train/valid split & shuffling:** Uses `np.random.default_rng(SEED=42)` to shuffle all examples deterministically, then performs an 80/20 split into `train_*` and `valid_*`.  \n",
        "- **Audio → log-mel features:**  \n",
        "  - **I/O:** `load_wav_16k_mono()` reads each file and decodes to mono at **16 kHz** (`SAMPLE_RATE=16000`).  \n",
        "  - **STFT/Mel:** `FRAME_LENGTH=512`, `FRAME_STEP=256`, projects the magnitude spectrogram to **128 mel bins** (`NUM_MELS=128`) over **80–7600 Hz**.  \n",
        "  - **Log-scale & min–max norm:** Applies `log(mel+1e-6)` then per-example min–max normalization.  \n",
        "  - **Shape standardization:** Adds a channel dim and pads/resizes to **(time=61, mel=128, channels=1)** so all inputs share a fixed size.  \n",
        "- **tf.data pipelines:**  \n",
        "  - **Training:** Builds `train_ds` by mapping `preprocess` → `cache()` → `shuffle(1000)` → `batch(16)` → `prefetch(AUTOTUNE)` for throughput.  \n",
        "  - **Validation:** Builds `valid_ds` similarly (without shuffle).  \n",
        "  - Prints one batch’s shapes to confirm tensors are `(batch, 61, 128, 1)` and labels `(batch,)`.  \n",
        "\n",
        "**Key hyperparameters:** `SAMPLE_RATE=16000`, `FRAME_LENGTH=512`, `FRAME_STEP=256`, `NUM_MELS=128`, `LOWER_HZ=80`, `UPPER_HZ=7600`, `TARGET_TIME_FRAMES=61`, `TARGET_MEL_BINS=128`, `BATCH_SIZE=16`, `SEED=42`.  \n",
        "\n",
        "**Purpose:** End-to-end data preparation—from Drive zip to balanced, batched, prefetched log-mel tensors—for training an Arabic letters classifier across 28 classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import uuid\n",
        "import subprocess\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential, callbacks, metrics\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
      ],
      "metadata": {
        "id": "dlU05wEivLf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ifvelAr_Iqr"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/QariAI/arabic_letters_dataset\"\n",
        "\n",
        "print(\"Dataset path:\", BASE_PATH)\n",
        "print(\"Number of subfolders:\", len(os.listdir(BASE_PATH)))\n",
        "print(\"First few subfolders:\", sorted(os.listdir(BASE_PATH))[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3isFp9j_tTC"
      },
      "outputs": [],
      "source": [
        "counts = {}\n",
        "for d in sorted(os.listdir(BASE_PATH)):\n",
        "    dpath = os.path.join(BASE_PATH, d)\n",
        "    if os.path.isdir(dpath):\n",
        "        wavs = glob.glob(os.path.join(dpath, \"*.wav\"))\n",
        "        counts[d] = len(wavs)\n",
        "\n",
        "print(\"Total classes found:\", len(counts))\n",
        "for k, v in counts.items():\n",
        "    print(f\"Class {k} -> {v} files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ifQlJlwAaYE"
      },
      "outputs": [],
      "source": [
        "CLASS_ID_TO_NAME = {\n",
        "    \"1\":\"01_A\",\"2\":\"02_Ba\",\"3\":\"03_Ta\",\"4\":\"04_Tsa\",\"5\":\"05_Ja\",\"6\":\"06_Hha\",\"7\":\"07_Kha\",\n",
        "    \"8\":\"08_Da\",\"9\":\"09_Dza\",\"10\":\"10_Ro\",\"11\":\"11_Za\",\"12\":\"12_Sa\",\"13\":\"13_Sya\",\"14\":\"14_Sho\",\n",
        "    \"15\":\"15_Dho\",\"16\":\"16_Tho\",\"17\":\"17_Zho\",\"18\":\"18_Ain\",\"19\":\"19_Gho\",\"20\":\"20_Fa\",\"21\":\"21_Qo\",\n",
        "    \"22\":\"22_Ka\",\"23\":\"23_La\",\"24\":\"24_Ma\",\"25\":\"25_Na\",\"26\":\"26_Ha\",\"27\":\"27_Wa\",\"28\":\"28_Ya\"\n",
        "}\n",
        "\n",
        "CLASS_IDS = [str(i) for i in range(1, 29)]\n",
        "CLASSES = [CLASS_ID_TO_NAME[cid] for cid in CLASS_IDS]\n",
        "CLASS_TO_INDEX = {name: i for i, name in enumerate(CLASSES)}\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "print(\"Classes (ordered):\", CLASSES)\n",
        "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
        "\n",
        "file_paths, labels = [], []\n",
        "per_class_counts = {}\n",
        "\n",
        "for idx, cid in enumerate(CLASS_IDS):\n",
        "    class_dir = os.path.join(BASE_PATH, cid)\n",
        "    wavs = sorted(glob.glob(os.path.join(class_dir, \"*.wav\")))\n",
        "    file_paths.extend(wavs)\n",
        "    labels.extend([idx] * len(wavs))\n",
        "    per_class_counts[CLASSES[idx]] = len(wavs)\n",
        "\n",
        "file_paths = np.array(file_paths)\n",
        "labels = np.array(labels, dtype=np.int64)\n",
        "\n",
        "print(f\"\\nTotal audio files: {len(file_paths)}\")\n",
        "print(\"Per-class counts:\")\n",
        "for k in CLASSES:\n",
        "    print(f\"{k:>7s} -> {per_class_counts.get(k, 0)}\")\n",
        "\n",
        "print(\"\\nSample items:\")\n",
        "for p, y in list(zip(file_paths, labels))[:5]:\n",
        "    print(y, \"->\", CLASSES[y], \"|\", p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8GBWgQRAovk"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "FRAME_LENGTH = 512\n",
        "FRAME_STEP   = 256\n",
        "NUM_MELS     = 128\n",
        "LOWER_HZ, UPPER_HZ = 80.0, 7600.0\n",
        "TARGET_TIME_FRAMES = 61\n",
        "TARGET_MEL_BINS    = 128\n",
        "BATCH_SIZE = 16\n",
        "SEED = 42\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "perm = rng.permutation(len(file_paths))\n",
        "file_paths = file_paths[perm]\n",
        "labels     = labels[perm]\n",
        "\n",
        "train_ratio = 0.8\n",
        "train_size = int(len(file_paths) * train_ratio)\n",
        "\n",
        "train_file_paths = file_paths[:train_size]\n",
        "train_labels     = labels[:train_size]\n",
        "valid_file_paths = file_paths[train_size:]\n",
        "valid_labels     = labels[train_size:]\n",
        "\n",
        "print(f\"Train size: {len(train_file_paths)} | Valid size: {len(valid_file_paths)}\")\n",
        "\n",
        "@tf.function\n",
        "def load_wav_16k_mono(file_path: tf.Tensor) -> tf.Tensor:\n",
        "    audio_binary = tf.io.read_file(file_path)\n",
        "    audio, _ = tf.audio.decode_wav(\n",
        "        audio_binary, desired_channels=1, desired_samples=SAMPLE_RATE\n",
        "    )\n",
        "    return tf.squeeze(audio, axis=-1)\n",
        "\n",
        "@tf.function\n",
        "def wav_to_logmelspec(wav: tf.Tensor) -> tf.Tensor:\n",
        "    stft = tf.signal.stft(wav, frame_length=FRAME_LENGTH, frame_step=FRAME_STEP)\n",
        "    spectrogram = tf.abs(stft)\n",
        "\n",
        "    num_spectrogram_bins = tf.shape(spectrogram)[-1]\n",
        "    mel_w = tf.signal.linear_to_mel_weight_matrix(\n",
        "        NUM_MELS, num_spectrogram_bins, SAMPLE_RATE, LOWER_HZ, UPPER_HZ\n",
        "    )\n",
        "    mel_spec = tf.tensordot(spectrogram, mel_w, axes=1)\n",
        "    mel_spec.set_shape(spectrogram.shape[:-1].concatenate(mel_w.shape[-1:]))\n",
        "\n",
        "    log_mel = tf.math.log(mel_spec + 1e-6)\n",
        "\n",
        "    min_v = tf.reduce_min(log_mel)\n",
        "    max_v = tf.reduce_max(log_mel)\n",
        "    log_mel = (log_mel - min_v) / (max_v - min_v + 1e-9)\n",
        "\n",
        "    log_mel = tf.expand_dims(log_mel, axis=-1)\n",
        "    log_mel = tf.image.resize_with_pad(log_mel, TARGET_TIME_FRAMES, TARGET_MEL_BINS)\n",
        "    return log_mel\n",
        "\n",
        "@tf.function\n",
        "def preprocess(file_path: tf.Tensor, label: tf.Tensor):\n",
        "    wav = load_wav_16k_mono(file_path)\n",
        "    spec = wav_to_logmelspec(wav)\n",
        "    return spec, label\n",
        "\n",
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((train_file_paths, train_labels))\n",
        "    .map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "    .cache()\n",
        "    .shuffle(1000, seed=SEED, reshuffle_each_iteration=True)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n",
        "\n",
        "valid_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((valid_file_paths, valid_labels))\n",
        "    .map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "    .cache()\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n",
        "\n",
        "for xb, yb in train_ds.take(1):\n",
        "    print(\"Batch image shape:\", xb.shape)\n",
        "    print(\"Batch labels shape:\", yb.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2brpXXl0tS6"
      },
      "source": [
        "# 2. CNN Model Training for Arabic **letters** classification\n",
        "\n",
        "This block defines and trains a convolutional neural network (CNN) using TensorFlow/Keras for classifying Arabic letters based on log-mel spectrogram inputs.\n",
        "\n",
        "- **Input setup:** Uses spectrogram tensors of shape **(61, 128, 1)** (time × mel bins × channel). Training runs for **100 epochs** (with early stopping).  \n",
        "- **Custom metrics:** Implements `MicroPrecision`, `MicroRecall`, and `MicroF1Score` as Keras metrics. These are simplified approximations that track correct vs incorrect predictions across all classes and report micro-averaged precision, recall, and F1 in addition to standard accuracy.  \n",
        "- **CNN architecture (Sequential):**  \n",
        "  - Conv2D(32) → MaxPool(2×2)  \n",
        "  - Conv2D(64) → MaxPool(2×2)  \n",
        "  - Conv2D(128) → MaxPool(2×2)  \n",
        "  - Conv2D(128) → MaxPool(2×2)  \n",
        "  - Flatten  \n",
        "  - Dense(512, ReLU) → Dropout(0.3)  \n",
        "  - Dense(NUM_CLASSES, Softmax) for final classification  \n",
        "- **Compilation:** Optimizer = **Adam**, loss = **sparse categorical crossentropy**, metrics = accuracy + custom precision/recall/F1.  \n",
        "- **Callbacks:**  \n",
        "  - `ModelCheckpoint` saves the best weights to `/content/model_hijaiyah_replica.h5` based on validation accuracy.  \n",
        "  - `EarlyStopping` monitors validation accuracy with **patience=10** and restores best weights.  \n",
        "- **Training:** Runs `model.fit()` on `train_ds`, validates on `valid_ds`, logs metrics each epoch, and saves the best-performing model.  \n",
        "- **Output:** Prints model summary before training and confirms the path of the best saved checkpoint.  \n",
        "\n",
        "**Purpose:** Builds a deep CNN to recognize 28 Arabic letters from spectrogram features, monitors training with both standard and custom evaluation metrics, and saves the best-performing model automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B24-TEswApJy"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = (61, 128, 1)\n",
        "EPOCHS = 100\n",
        "\n",
        "class MicroPrecision(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='micro_precision', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
        "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.argmax(y_pred, axis=-1)\n",
        "        y_true = tf.cast(y_true, tf.int64)\n",
        "        y_pred = tf.cast(y_pred, tf.int64)\n",
        "        correct = tf.cast(tf.equal(y_true, y_pred), tf.float32)\n",
        "        self.tp.assign_add(tf.reduce_sum(correct))\n",
        "        self.fp.assign_add(tf.reduce_sum(1.0 - correct))\n",
        "\n",
        "    def result(self):\n",
        "        return self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n",
        "\n",
        "class MicroRecall(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='micro_recall', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
        "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.argmax(y_pred, axis=-1)\n",
        "        y_true = tf.cast(y_true, tf.int64)\n",
        "        y_pred = tf.cast(y_pred, tf.int64)\n",
        "        correct = tf.cast(tf.equal(y_true, y_pred), tf.float32)\n",
        "        self.tp.assign_add(tf.reduce_sum(correct))\n",
        "        self.fn.assign_add(tf.reduce_sum(\n",
        "            tf.cast(tf.logical_and(tf.not_equal(y_true, y_pred), tf.equal(y_true, 0)), tf.float32)\n",
        "        ))\n",
        "\n",
        "    def result(self):\n",
        "        return self.tp / (self.tp + self.fn + tf.keras.backend.epsilon())\n",
        "\n",
        "class MicroF1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='micro_f1_score', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.p = MicroPrecision()\n",
        "        self.r = MicroRecall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.p.update_state(y_true, y_pred)\n",
        "        self.r.update_state(y_true, y_pred)\n",
        "\n",
        "    def result(self):\n",
        "        p = self.p.result()\n",
        "        r = self.r.result()\n",
        "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=INPUT_SHAPE),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy', MicroPrecision(), MicroRecall(), MicroF1Score()]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "ckpt_path = \"/content/model_hijaiyah_replica.h5\"\n",
        "cbs = [\n",
        "    callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
        "    callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10, restore_best_weights=True, verbose=1)\n",
        "]\n",
        "\n",
        "hist = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_ds,\n",
        "    callbacks=cbs,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Best model saved to:\", ckpt_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-Kczxtm93Cq"
      },
      "source": [
        "# 3. Evaluation, diagnostics, and on-device inference utilities (Arabic **letters** model)\n",
        "\n",
        "This block evaluates the trained CNN on the validation set, visualizes performance, computes Top-k metrics, and provides convenient **file-level prediction** utilities (including auto-conversion to 16 kHz mono WAV via `ffmpeg`) plus a Colab file-upload flow.\n",
        "\n",
        "- **Validation metrics & reports**\n",
        "  - Runs `model.evaluate(valid_ds)` and prints all compiled metrics.\n",
        "  - Collects predictions over `valid_ds` to generate:\n",
        "    - `classification_report` (per-class precision/recall/F1 using `CLASSES` names).\n",
        "    - **Confusion matrix** (raw and row-normalized), with a heatmap (Matplotlib).\n",
        "\n",
        "- **Top-k accuracy**\n",
        "  - Computes **Top-1** (argmax) and **Top-5** accuracies from predicted probability vectors.\n",
        "  - Utility `topk_acc()` provided for generic k.\n",
        "\n",
        "- **Single-file prediction helpers**\n",
        "  - `_load_wav_16k()` and `predict_file(path)` → fast path for already-16kHz mono WAV.\n",
        "  - `convert_to_wav16k_mono(in_path)` uses **ffmpeg** to convert arbitrary audio (`.wav/.m4a/.mp3`) to 16 kHz mono WAV (saved in `/content/converted`).\n",
        "  - `_load_wav_exact_1s()` pads/trims to exactly 1.0s (16,000 samples) for consistent features.\n",
        "  - `predict_file_any_audio(in_path)` returns **top-1 label + confidence** and a **Top-5 dict**.\n",
        "\n",
        "- **Interactive Colab upload**\n",
        "  - Prompts to upload one or more audio files, auto-converts, plays back the converted WAV, and prints predictions with Top-5 probabilities.\n",
        "\n",
        "- **Figures & history export**\n",
        "  - Creates `/content/figures_letters/` and saves:\n",
        "    - Accuracy and loss curves (`letters_acc_curve.png`, `letters_loss_curve.png`).\n",
        "    - Confusion matrix (`letters_confusion_matrix.png`).\n",
        "    - Raw Keras `history.history` as JSON (`letters_history.json`) for later analysis.\n",
        "\n",
        "**Purpose:** End-to-end post-training analysis and demo tooling—quantitative metrics, visual diagnostics, reproducible figure exports, and quick ad-hoc inference on user-provided audio—so you can validate model quality and showcase predictions directly in Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDkum4I2CNaZ"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(valid_ds, verbose=1)\n",
        "print(\"\\nMetrics on validation set:\")\n",
        "for name, val in zip(model.metrics_names, results):\n",
        "    print(f\"  {name:>22s}: {val:.4f}\")\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for xb, yb in valid_ds:\n",
        "    probs = model.predict(xb, verbose=0)\n",
        "    y_pred.extend(np.argmax(probs, axis=1))\n",
        "    y_true.extend(yb.numpy())\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "all_labels = np.arange(NUM_CLASSES)\n",
        "\n",
        "present = np.unique(y_true)\n",
        "missing = sorted(set(all_labels) - set(present))\n",
        "print(\"\\nLabel diagnostics:\")\n",
        "print(\"  Present labels: \", present.tolist())\n",
        "print(\"  Missing labels: \", missing)\n",
        "if len(CLASSES) != NUM_CLASSES:\n",
        "    print(f\"⚠️  CLASSES length ({len(CLASSES)}) != NUM_CLASSES ({NUM_CLASSES})\")\n",
        "\n",
        "print(\"\\nClassification report (per class):\\n\")\n",
        "print(classification_report(\n",
        "    y_true, y_pred,\n",
        "    labels=all_labels,\n",
        "    target_names=CLASSES,\n",
        "    digits=3,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=all_labels)\n",
        "cm_norm = cm / (cm.sum(axis=1, keepdims=True) + 1e-9)\n",
        "\n",
        "plt.figure(figsize=(10, 9))\n",
        "plt.imshow(cm_norm, interpolation='nearest', cmap='Blues', aspect='auto')\n",
        "plt.title(\"Confusion Matrix (normalized)\")\n",
        "plt.colorbar()\n",
        "ticks = np.arange(NUM_CLASSES)\n",
        "plt.xticks(ticks, CLASSES, rotation=90)\n",
        "plt.yticks(ticks, CLASSES)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "confusions = []\n",
        "for i in range(NUM_CLASSES):\n",
        "    for j in range(NUM_CLASSES):\n",
        "        if i != j and cm[i, j] > 0:\n",
        "            confusions.append((CLASSES[i], CLASSES[j], int(cm[i, j])))\n",
        "confusions.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "print(\"\\nTop confusions (true -> predicted, count):\")\n",
        "for t, p, c in confusions[:20]:\n",
        "    print(f\"{t:>7s} -> {p:<7s} : {c}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R0TBcHqGsO-"
      },
      "outputs": [],
      "source": [
        "top1_correct = 0\n",
        "top5_correct = 0\n",
        "n = 0\n",
        "\n",
        "for xb, yb in valid_ds:\n",
        "    probs = model.predict(xb, verbose=0)\n",
        "    y_true = yb.numpy()\n",
        "    top1 = np.argmax(probs, axis=1)\n",
        "    top5 = np.argsort(-probs, axis=1)[:, :5]\n",
        "\n",
        "    top1_correct += (top1 == y_true).sum()\n",
        "    top5_correct += sum(y in top5[i] for i, y in enumerate(y_true))\n",
        "    n += y_true.shape[0]\n",
        "\n",
        "print(f\"Top-1 accuracy: {top1_correct/n:.3f}\")\n",
        "print(f\"Top-5 accuracy: {top5_correct/n:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK-k9krrGvAu"
      },
      "outputs": [],
      "source": [
        "def _load_wav_16k(path):\n",
        "    audio_binary = tf.io.read_file(path)\n",
        "    audio, _ = tf.audio.decode_wav(audio_binary, desired_channels=1, desired_samples=16000)\n",
        "    return tf.squeeze(audio, -1)\n",
        "\n",
        "def predict_file(path, model=model):\n",
        "    wav = _load_wav_16k(path)\n",
        "    spec = wav_to_logmelspec(wav)\n",
        "    spec = tf.expand_dims(spec, 0)\n",
        "    probs = model.predict(spec, verbose=0)[0]\n",
        "    top_idx = int(np.argmax(probs))\n",
        "    top_label = CLASSES[top_idx]\n",
        "    top_conf = float(probs[top_idx])\n",
        "\n",
        "    order = np.argsort(-probs)[:5]\n",
        "    top5 = {CLASSES[i]: float(probs[i]) for i in order}\n",
        "    return top_label, top_conf, top5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ2VGVt3GxwN"
      },
      "outputs": [],
      "source": [
        "CONVERT_DIR = \"/content/converted\"\n",
        "os.makedirs(CONVERT_DIR, exist_ok=True)\n",
        "\n",
        "def convert_to_wav16k_mono(in_path: str) -> str:\n",
        "    \"\"\"Use ffmpeg to convert to 16 kHz mono WAV. Returns output path.\"\"\"\n",
        "    base = os.path.splitext(os.path.basename(in_path))[0]\n",
        "    out_path = os.path.join(CONVERT_DIR, f\"{base}_{uuid.uuid4().hex[:8]}.wav\")\n",
        "    cmd = [\"ffmpeg\", \"-y\", \"-i\", in_path, \"-ac\", \"1\", \"-ar\", \"16000\", out_path]\n",
        "    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    if proc.returncode != 0 or not os.path.exists(out_path):\n",
        "        raise RuntimeError(f\"ffmpeg failed converting {in_path} -> WAV:\\n{proc.stderr.decode(errors='ignore')}\")\n",
        "    return out_path\n",
        "\n",
        "def _load_wav_exact_1s(path: str) -> tf.Tensor:\n",
        "    \"\"\"Load WAV as 16k mono and pad/trim to exactly 1.0 s (16000 samples).\"\"\"\n",
        "    audio_binary = tf.io.read_file(path)\n",
        "    audio, sr = tf.audio.decode_wav(audio_binary, desired_channels=1)\n",
        "    wav = tf.squeeze(audio, -1)\n",
        "    target = 16000\n",
        "    n = tf.shape(wav)[0]\n",
        "    def trim():\n",
        "        return wav[:target]\n",
        "    def pad():\n",
        "        pad_len = target - n\n",
        "        return tf.pad(wav, [[0, pad_len]])\n",
        "    wav = tf.cond(n >= target, trim, pad)\n",
        "    return wav\n",
        "\n",
        "def predict_file_any_audio(in_path: str, model=model):\n",
        "    \"\"\"Convert arbitrary audio to 16k WAV, make log-mel, predict top-1 & top-5.\"\"\"\n",
        "    wav_path = convert_to_wav16k_mono(in_path)\n",
        "    wav = _load_wav_exact_1s(wav_path)\n",
        "    spec = wav_to_logmelspec(wav)\n",
        "    spec = tf.expand_dims(spec, 0)\n",
        "    probs = model.predict(spec, verbose=0)[0]\n",
        "    top_idx = int(np.argmax(probs))\n",
        "    top_label = CLASSES[top_idx]\n",
        "    top_conf = float(probs[top_idx])\n",
        "    order = np.argsort(-probs)[:5]\n",
        "    top5 = {CLASSES[i]: float(probs[i]) for i in order}\n",
        "    return top_label, top_conf, top5, wav_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oralSe47H9l_"
      },
      "outputs": [],
      "source": [
        "print(\"📤 Select one or more audio files (.wav, .m4a, .mp3). I'll convert to 16 kHz mono WAV.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fname, _ in uploaded.items():\n",
        "    in_path = f\"/content/{fname}\"\n",
        "    try:\n",
        "        label, conf, top5, wav16_path = predict_file_any_audio(in_path)\n",
        "        print(\"\\n────────────\")\n",
        "        print(f\"🎧 {fname}  →  converted: {os.path.basename(wav16_path)}\")\n",
        "        display(Audio(wav16_path))\n",
        "        print(f\"🔮 Prediction: {label}  ({conf:.2%})\")\n",
        "        print(\"🏅 Top‑5:\")\n",
        "        for k, v in top5.items():\n",
        "            print(f\"   - {k:10s}: {v:.2%}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error on {fname}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkWjuw9XuqiT"
      },
      "outputs": [],
      "source": [
        "FIG_DIR = \"/content/figures_letters\"\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "\n",
        "def to_numpy_labels(y):\n",
        "    \"\"\"Accept one-hot or integer labels -> return integer labels.\"\"\"\n",
        "    y = np.asarray(y)\n",
        "    if y.ndim > 1 and y.shape[-1] > 1:\n",
        "        return np.argmax(y, axis=-1)\n",
        "    return y\n",
        "\n",
        "def collect_from_dataset(ds):\n",
        "    \"\"\"Collect features and integer labels from a tf.data dataset.\"\"\"\n",
        "    ys_true, ys_prob = [], []\n",
        "    for batch in ds:\n",
        "        if isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
        "            x, y = batch\n",
        "        else:\n",
        "            raise ValueError(\"val_ds should yield (x, y)\")\n",
        "        probs = model.predict(x, verbose=0)\n",
        "        ys_prob.append(probs)\n",
        "        ys_true.append(y.numpy() if hasattr(y, \"numpy\") else y)\n",
        "    y_true = to_numpy_labels(np.concatenate(ys_true, axis=0))\n",
        "    y_prob = np.concatenate(ys_prob, axis=0)\n",
        "    return y_true, y_prob\n",
        "\n",
        "def plot_training_curves(history, title_prefix=\"Letters Model\"):\n",
        "    h = history.history\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(h.get(\"accuracy\", []), label=\"train acc\")\n",
        "    plt.plot(h.get(\"val_accuracy\", []), label=\"val acc\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend()\n",
        "    plt.title(f\"{title_prefix}: Accuracy\")\n",
        "    acc_path = os.path.join(FIG_DIR, \"letters_acc_curve.png\")\n",
        "    plt.tight_layout(); plt.savefig(acc_path, dpi=300); plt.show()\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(h.get(\"loss\", []), label=\"train loss\")\n",
        "    plt.plot(h.get(\"val_loss\", []), label=\"val loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
        "    plt.title(f\"{title_prefix}: Loss\")\n",
        "    loss_path = os.path.join(FIG_DIR, \"letters_loss_curve.png\")\n",
        "    plt.tight_layout(); plt.savefig(loss_path, dpi=300); plt.show()\n",
        "    return acc_path, loss_path\n",
        "\n",
        "def plot_confmat(cm, class_names, normalize=True, title=\"Letters Confusion Matrix\"):\n",
        "    if normalize:\n",
        "        cm = cm.astype(\"float\") / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    im = plt.imshow(cm, interpolation=\"nearest\", aspect=\"auto\")\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.xticks(ticks=np.arange(len(class_names)), labels=class_names, rotation=90)\n",
        "    plt.yticks(ticks=np.arange(len(class_names)), labels=class_names)\n",
        "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    path = os.path.join(FIG_DIR, \"letters_confusion_matrix.png\")\n",
        "    plt.savefig(path, dpi=300); plt.show()\n",
        "    return path\n",
        "\n",
        "def topk_accuracy(y_true, y_prob, k=5):\n",
        "    \"\"\"Compute Top-k accuracy given integer y_true and probability matrix y_prob.\"\"\"\n",
        "    topk = np.argsort(-y_prob, axis=1)[:, :k]\n",
        "    match = (topk == y_true.reshape(-1,1)).any(axis=1)\n",
        "    return match.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JS7Eym_us2l"
      },
      "outputs": [],
      "source": [
        "FIG_DIR = \"/content/figures_letters\"; os.makedirs(FIG_DIR, exist_ok=True)\n",
        "\n",
        "def plot_and_save(history, key, title, fname):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(history.history[key], label=f\"train {key}\")\n",
        "    plt.plot(history.history[f\"val_{key}\"], label=f\"val {key}\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(key.capitalize()); plt.legend()\n",
        "    plt.title(title); plt.tight_layout()\n",
        "    path = os.path.join(FIG_DIR, fname); plt.savefig(path, dpi=300); plt.show()\n",
        "    return path\n",
        "\n",
        "acc_png = plot_and_save(hist, \"accuracy\", \"Letters model: accuracy\", \"letters_acc_curve.png\")\n",
        "loss_png = plot_and_save(hist, \"loss\", \"Letters model: loss\", \"letters_loss_curve.png\")\n",
        "print(\"Saved:\", acc_png, loss_png)\n",
        "\n",
        "import json, pathlib\n",
        "with open(pathlib.Path(FIG_DIR)/\"letters_history.json\",\"w\") as f:\n",
        "    json.dump(hist.history, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtj7rZWjvnRZ"
      },
      "outputs": [],
      "source": [
        "y_true, y_prob = [], []\n",
        "for x_batch, y_batch in valid_ds:\n",
        "    probs = model.predict(x_batch, verbose=0)\n",
        "    y_prob.append(probs)\n",
        "    y_true.append(y_batch.numpy())\n",
        "y_prob = np.concatenate(y_prob, axis=0)\n",
        "y_true = np.concatenate(y_true, axis=0)\n",
        "y_pred = np.argmax(y_prob, axis=1)\n",
        "\n",
        "def topk_acc(y_true, y_prob, k=5):\n",
        "    topk = np.argsort(-y_prob, axis=1)[:, :k]\n",
        "    return np.mean([yt in row for yt, row in zip(y_true, topk)])\n",
        "\n",
        "top1 = accuracy_score(y_true, y_pred)\n",
        "top5 = topk_acc(y_true, y_prob, k=5)\n",
        "print(f\"Top-1: {top1:.3f} | Top-5: {top5:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
